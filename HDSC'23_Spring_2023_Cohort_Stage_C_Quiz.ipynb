{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMyygrH/ZgcNdcHrztPl1tW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dullard-boy/Hamoye-Data-Science-Internship-2023/blob/main/HDSC'23_Spring_2023_Cohort_Stage_C_Quiz.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "RrR8Fq6u11CN"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "data=pd.read_csv('/content/Data_for_UCI_named.csv')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Osz_LYUN27hx",
        "outputId": "5ef3d789-28d0-4cdf-e029-d54eb9dcae94"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['tau1', 'tau2', 'tau3', 'tau4', 'p1', 'p2', 'p3', 'p4', 'g1', 'g2',\n",
              "       'g3', 'g4', 'stab', 'stabf'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "# Convert the target variable to numeric\n",
        "le = LabelEncoder()\n",
        "data['stabf'] = le.fit_transform(data['stabf'])\n",
        "\n",
        "# Split the data into train and test sets\n",
        "X = data[['tau1', 'tau2', 'tau3', 'tau4', 'p1', 'p2', 'p3', 'p4', 'g1', 'g2', 'g3', 'g4']]\n",
        "y = data['stabf']\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
        "\n",
        "# Scale the features\n",
        "scaler = StandardScaler()\n",
        "x_train_scaled = scaler.fit_transform(x_train)\n",
        "x_test_scaled = scaler.transform(x_test)\n",
        "\n",
        "# Train a Random Forest Classifier\n",
        "rf_clf = RandomForestClassifier(random_state=1)\n",
        "rf_clf.fit(x_train_scaled, y_train)\n",
        "rf_predictions = rf_clf.predict(x_test_scaled)\n",
        "rf_accuracy = accuracy_score(y_test, rf_predictions)\n",
        "print(\"Random Forest Classifier Accuracy:\", rf_accuracy)\n",
        "\n",
        "# Train an Extra Trees Classifier\n",
        "et_clf = ExtraTreesClassifier(random_state=1)\n",
        "et_clf.fit(x_train_scaled, y_train)\n",
        "et_predictions = et_clf.predict(x_test_scaled)\n",
        "et_accuracy = accuracy_score(y_test, et_predictions)\n",
        "print(\"Extra Trees Classifier Accuracy:\", et_accuracy)\n",
        "\n",
        "# Train an XGBoost Classifier\n",
        "xgb_clf = xgb.XGBClassifier(random_state=1)\n",
        "xgb_clf.fit(x_train_scaled, y_train)\n",
        "xgb_predictions = xgb_clf.predict(x_test_scaled)\n",
        "xgb_accuracy = accuracy_score(y_test, xgb_predictions)\n",
        "print(\"XGBoost Classifier Accuracy:\", xgb_accuracy)\n",
        "\n",
        "# Train a LightGBM Classifier\n",
        "lgb_clf = lgb.LGBMClassifier(random_state=1)\n",
        "lgb_clf.fit(x_train_scaled, y_train)\n",
        "lgb_predictions = lgb_clf.predict(x_test_scaled)\n",
        "lgb_accuracy = accuracy_score(y_test, lgb_predictions)\n",
        "print(\"LightGBM Classifier Accuracy:\", lgb_accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "acvJ6buv52ob",
        "outputId": "6023cd6f-d925-482b-de10-5a0f47527409"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest Classifier Accuracy: 0.929\n",
            "Extra Trees Classifier Accuracy: 0.928\n",
            "XGBoost Classifier Accuracy: 0.9455\n",
            "LightGBM Classifier Accuracy: 0.9395\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve the feature importances\n",
        "feature_importances = et_clf.feature_importances_\n",
        "\n",
        "# Create a dataframe to display the feature importance values\n",
        "importance_df = pd.DataFrame({'Feature': X.columns, 'Importance': feature_importances})\n",
        "importance_df = importance_df.sort_values('Importance', ascending=False)\n",
        "\n",
        "# Display the most and least important features\n",
        "most_important_feature = importance_df.iloc[0]['Feature']\n",
        "least_important_feature = importance_df.iloc[-1]['Feature']\n",
        "\n",
        "print(\"Most important feature:\", most_important_feature)\n",
        "print(\"Least important feature:\", least_important_feature)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3NujZH1V7_w_",
        "outputId": "abda273c-411b-4a69-a4a4-d82baae4bfd0"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most important feature: tau2\n",
            "Least important feature: p1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "\n",
        "# Split the data into train and test sets\n",
        "X = data[['tau1', 'tau2', 'tau3', 'tau4', 'p1', 'p2', 'p3', 'p4', 'g1', 'g2', 'g3', 'g4']]\n",
        "y = data['stabf']\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
        "\n",
        "# Scale the features\n",
        "scaler = StandardScaler()\n",
        "x_train_scaled = scaler.fit_transform(x_train)\n",
        "x_test_scaled = scaler.transform(x_test)\n",
        "\n",
        "# Define the parameter grid for RandomizedSearchCV\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'max_depth': [None, 5, 10],\n",
        "    # Add other hyperparameters as per your RandomizedSearchCV results\n",
        "}\n",
        "\n",
        "# Train a new Extra Trees Classifier with the optimized hyperparameters\n",
        "et_clf_optimal = ExtraTreesClassifier(random_state=1)\n",
        "random_search = RandomizedSearchCV(estimator=et_clf_optimal, param_distributions=param_grid, random_state=1)\n",
        "random_search.fit(x_train_scaled, y_train)\n",
        "\n",
        "# Evaluate the new optimal model on the test set\n",
        "optimal_predictions = random_search.predict(x_test_scaled)\n",
        "optimal_accuracy = accuracy_score(y_test, optimal_predictions)\n",
        "\n",
        "# Compare the accuracy of the new optimal model with the initial model\n",
        "initial_accuracy = et_clf.score(x_test_scaled, y_test)\n",
        "\n",
        "if optimal_accuracy > initial_accuracy:\n",
        "    print(\"The accuracy of the new optimal model is higher than the initial model.\")\n",
        "elif optimal_accuracy < initial_accuracy:\n",
        "    print(\"The accuracy of the new optimal model is lower than the initial model.\")\n",
        "else:\n",
        "    print(\"The accuracy of the new optimal model is the same as the initial model.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "diErN88q-DnR",
        "outputId": "01e08f86-fc72-4f77-c0ba-e909bb5ec4ca"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:305: UserWarning: The total space of parameters 9 is smaller than n_iter=10. Running 9 iterations. For exhaustive searches, use GridSearchCV.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The accuracy of the new optimal model is higher than the initial model.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Split the data into X (features) and y (target)\n",
        "X = data[['tau1', 'tau2', 'tau3', 'tau4', 'p1', 'p2', 'p3', 'p4', 'g1', 'g2', 'g3', 'g4']]\n",
        "y = data['stabf']\n",
        "\n",
        "# Define the parameter grid for RandomizedSearchCV\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'max_depth': [None, 5, 10],\n",
        "    # Add other hyperparameters you want to tune\n",
        "}\n",
        "\n",
        "# Create an ExtraTreesClassifier estimator\n",
        "et_clf = ExtraTreesClassifier(random_state=1)\n",
        "\n",
        "# Create a RandomizedSearchCV object\n",
        "random_search = RandomizedSearchCV(estimator=et_clf, param_distributions=param_grid,\n",
        "                                   scoring='accuracy', cv=5, n_iter=10, n_jobs=-1,\n",
        "                                   verbose=1, random_state=1)\n",
        "\n",
        "# Fit the RandomizedSearchCV object to the data\n",
        "random_search.fit(X, y)\n",
        "\n",
        "# Get the best parameters found by RandomizedSearchCV\n",
        "best_params = random_search.best_params_\n",
        "\n",
        "print(\"Best Hyperparameters:\")\n",
        "print(best_params)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DLu7viIm-mu6",
        "outputId": "c390fa68-1353-42a3-849b-327ceed5941c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:305: UserWarning: The total space of parameters 9 is smaller than n_iter=10. Running 9 iterations. For exhaustive searches, use GridSearchCV.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n",
            "Best Hyperparameters:\n",
            "{'n_estimators': 300, 'max_depth': None}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9FzV_zrS_JtX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}